########################################
###          DEFAULT SETTINGS        ###
########################################

# Any of those settings can be changed
# by the user. To overwrite a setting, 
# create a settings.toml or load the 
# settings from .env file or vars.
# As an example, to disable the 
# myllm object:
# settings.toml
[default]
# Dynaconf settings verification
# use for testing
VALUE = "On default" 

# Module Enable/Disable
myllm_enabled = true
ai_agent_mode = false
ai_agent_prefix = "üß†" # prefix use to filter the AI response
ai_agent_suffix = "" # suffix use to filter the AI response

[default.myllm.template]
library = "g4f" # options are openai or g4f
enabled = false # options are true or false to enable or disable the llm
llm_model= "" # model to use e.g. gpt-3.5-turbo, gpt-4, gpt-4-32k
llm_provider = "g4f.Provider.Llama" # only for g4f. Refer to https://github.com/xtekky/gpt4free
llm_provider_key = "" # only for bard and openai to pass either the cookie or the api key
max_memory = 100 # Conversation history size
load_history = false # load conversation history via a json file
timeout = 2 # time lag to wait ai response
stream_mode = false # enable streaming mode NOT IMPLEMENTED
llm_prefix = "üêª" # prefix use to filter the AI response
llm_template = """
You are a friendly AI, helping me with general tasks. 
Be simple and direct.
"""

# [default.myllm.gpt] 
# enabled = true
# llm_library = "openai"
# llm_model= "gpt-4o"
# llm_provider = "" 
# llm_provider_key = "DEADBE4F"
# max_memory = 100 
# timeout = 2
# load_history = false
# stream_mode = false # enable streaming mode
# llm_prefix = ""
# llm_template = """
# You are a friendly AI, helping me with general tasks. 
# """

# [default.myllm.groq]
# library = "groq" # options are openai or g4f
# enabled = true # options are true or false to enable or disable the llm
# llm_model= "llama3-70b-8192" # model to use e.g. gpt-3.5-turbo, gpt-4, gpt-4-32k
# llm_provider = "" # only for g4f. Refer to https://github.com/xtekky/gpt4free
# llm_provider_key = "DeadBeef" # only for bard and openai to pass either the cookie or the api key
# max_memory = 11000 # Conversation history size
# load_history = false # load conversation history via a json file
# timeout = 1 # time lag to wait ai response
# stream_mode = false # enable streaming mode NOT IMPLEMENTED
# llm_prefix = "üêª" # prefix use to filter the AI response
# llm_template = """
# You are a friendly AI, helping me with general tasks. 
# """

# [default.myllm.localai] 
# library = "openai"
# enabled = true
# llm_model= "" 
# llm_provider = "" 
# llm_provider_key = "DEADBE4F"
# llm_base_url = "http://localhost:8080"
# max_memory = 100
# load_history = false
# timeout = 1 
# stream_mode = false # enable streaming mode
# llm_prefix = ""
# llm_template = """
# You are a friendly AI, helping me with 
# general tasks.
# """



########################################
###     END OF DEFAULT SETTINGS      ###
########################################